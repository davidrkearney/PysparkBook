# Handling Missing Data with Pyspark

from pyspark.sql import SparkSession
from pyspark.sql.functions import countDistinct, avg,stddev

# Load data from a CSV
file_location = "/FileStore/tables/df_panel_fix.csv"
df = spark.read.format("CSV").option("inferSchema", True).option("header", True).load(file_location)
display(df.take(5))

df.show()

## Dropping Columns without non-null values

# Has to have at least 2 NON-null values
df.na.drop(thresh=2).show()

## Dropping any row that contains missing data

df.na.drop().show()

df.na.drop(subset=["general"]).show()

df.na.drop(how='any').show()

df.na.drop(how='all').show()

## Imputation of Null Values

df.na.fill('example').show()

### Imputation of 0

df.na.fill(0).show()

df.na.fill('example',subset=['fr']).show()

df.na.fill(0,subset=['general']).show()

### Imputation of the Mean

from pyspark.sql.functions import mean
mean_val = df.select(mean(df['general'])).collect()

mean_val[0][0]

mean_gen = mean_val[0][0]

df.na.fill(mean_gen,["general"]).show()

df.na.fill(df.select(mean(df['general'])).collect()[0][0],['general']).show()