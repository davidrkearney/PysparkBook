# Group By and Aggregation with Pyspark

> "Group By and Aggregation with Pyspark"

- toc: true- branch: master- badges: true
- comments: true
- author: David Kearney
- categories: [pyspark, jupyter]
- description: Group By and Aggregation with Pyspark
- title: Group By and Aggregation with Pyspark

## Read CSV and inferSchema

from pyspark.sql import SparkSession
from pyspark.sql.functions import countDistinct, avg,stddev


# Load data from a CSV
file_location = "/FileStore/tables/df_panel_fix.csv"
df = spark.read.format("CSV").option("inferSchema", True).option("header", True).load(file_location)
display(df.take(5))


df.printSchema()

## Using groupBy for Averages and Counts

df.groupBy("province")

df.groupBy("province").mean().show()

df.groupBy("reg").mean().show()

# Count
df.groupBy("reg").count().show()

# Max
df.groupBy("reg").max().show()

# Min
df.groupBy("reg").min().show()

# Sum
df.groupBy("reg").sum().show()

# Max it across everything
df.agg({'specific':'max'}).show()

grouped = df.groupBy("reg")
grouped.agg({"it":'max'}).show()

df.select(countDistinct("reg")).show()

df.select(countDistinct("reg").alias("Distinct Region")).show()

df.select(avg('specific')).show()

df.select(stddev("specific")).show()

## Choosing Significant Digits with format_number

from pyspark.sql.functions import format_number


specific_std = df.select(stddev("specific").alias('std'))
specific_std.show()

specific_std.select(format_number('std',0)).show()

## Using orderBy

df.orderBy("specific").show()

df.orderBy(df["specific"].desc()).show()
