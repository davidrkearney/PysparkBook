\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\citation{markdown:kearney2019ties}
\newlabel{intro::doc}{{}{1}{}{section*.2}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Resilient Distributed Datasets}{3}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{intro:resilient-distributed-datasets}{{1}{3}{Resilient Distributed Datasets}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Pyspark Regression with Fiscal Data}{3}{section.1.1}}
\newlabel{2020-08-15-Pyspark-Fiscal-Data-Regression:pyspark-regression-with-fiscal-data}{{1.1}{3}{Pyspark Regression with Fiscal Data}{section.1.1}{}}
\newlabel{2020-08-15-Pyspark-Fiscal-Data-Regression::doc}{{1.1}{3}{Pyspark Regression with Fiscal Data}{section.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Bring in needed imports}{3}{subsection.1.1.1}}
\newlabel{2020-08-15-Pyspark-Fiscal-Data-Regression:bring-in-needed-imports}{{1.1.1}{3}{Bring in needed imports}{subsection.1.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Load data from CSV}{3}{subsection.1.1.2}}
\newlabel{2020-08-15-Pyspark-Fiscal-Data-Regression:load-data-from-csv}{{1.1.2}{3}{Load data from CSV}{subsection.1.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Describing the Data}{4}{subsection.1.1.3}}
\newlabel{2020-08-15-Pyspark-Fiscal-Data-Regression:describing-the-data}{{1.1.3}{4}{Describing the Data}{subsection.1.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}Cast Data Type}{4}{subsection.1.1.4}}
\newlabel{2020-08-15-Pyspark-Fiscal-Data-Regression:cast-data-type}{{1.1.4}{4}{Cast Data Type}{subsection.1.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.5}printSchema}{4}{subsection.1.1.5}}
\newlabel{2020-08-15-Pyspark-Fiscal-Data-Regression:printschema}{{1.1.5}{4}{printSchema}{subsection.1.1.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.6}Linear Regression in Pyspark}{4}{subsection.1.1.6}}
\newlabel{2020-08-15-Pyspark-Fiscal-Data-Regression:linear-regression-in-pyspark}{{1.1.6}{4}{Linear Regression in Pyspark}{subsection.1.1.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Group By and Aggregation with Pyspark}{5}{section.1.2}}
\newlabel{2020-08-17-Pyspark-Group-By:group-by-and-aggregation-with-pyspark}{{1.2}{5}{Group By and Aggregation with Pyspark}{section.1.2}{}}
\newlabel{2020-08-17-Pyspark-Group-By::doc}{{1.2}{5}{Group By and Aggregation with Pyspark}{section.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Read CSV and inferSchema}{6}{subsection.1.2.1}}
\newlabel{2020-08-17-Pyspark-Group-By:read-csv-and-inferschema}{{1.2.1}{6}{Read CSV and inferSchema}{subsection.1.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Using groupBy for Averages and Counts}{6}{subsection.1.2.2}}
\newlabel{2020-08-17-Pyspark-Group-By:using-groupby-for-averages-and-counts}{{1.2.2}{6}{Using groupBy for Averages and Counts}{subsection.1.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Choosing Significant Digits with format\_number}{7}{subsection.1.2.3}}
\newlabel{2020-08-17-Pyspark-Group-By:choosing-significant-digits-with-format-number}{{1.2.3}{7}{Choosing Significant Digits with format\_number}{subsection.1.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Using orderBy}{7}{subsection.1.2.4}}
\newlabel{2020-08-17-Pyspark-Group-By:using-orderby}{{1.2.4}{7}{Using orderBy}{subsection.1.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Handling Missing Data with Pyspark}{7}{section.1.3}}
\newlabel{2020-08-18-Pyspark-NAs:handling-missing-data-with-pyspark}{{1.3}{7}{Handling Missing Data with Pyspark}{section.1.3}{}}
\newlabel{2020-08-18-Pyspark-NAs::doc}{{1.3}{7}{Handling Missing Data with Pyspark}{section.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Dropping Columns without non\sphinxhyphen {}null values}{7}{subsection.1.3.1}}
\newlabel{2020-08-18-Pyspark-NAs:dropping-columns-without-non-null-values}{{1.3.1}{7}{Dropping Columns without non\sphinxhyphen {}null values}{subsection.1.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Dropping any row that contains missing data}{7}{subsection.1.3.2}}
\newlabel{2020-08-18-Pyspark-NAs:dropping-any-row-that-contains-missing-data}{{1.3.2}{7}{Dropping any row that contains missing data}{subsection.1.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}Imputation of Null Values}{8}{subsection.1.3.3}}
\newlabel{2020-08-18-Pyspark-NAs:imputation-of-null-values}{{1.3.3}{8}{Imputation of Null Values}{subsection.1.3.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Imputation of 0}{8}{subsubsection*.3}}
\newlabel{2020-08-18-Pyspark-NAs:imputation-of-0}{{1.3.3}{8}{Imputation of 0}{subsubsection*.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Imputation of the Mean}{8}{subsubsection*.4}}
\newlabel{2020-08-18-Pyspark-NAs:imputation-of-the-mean}{{1.3.3}{8}{Imputation of the Mean}{subsubsection*.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Dataframe Filitering and Operations with Pyspark}{8}{section.1.4}}
\newlabel{2020-08-19-Pyspark-Filtering:dataframe-filitering-and-operations-with-pyspark}{{1.4}{8}{Dataframe Filitering and Operations with Pyspark}{section.1.4}{}}
\newlabel{2020-08-19-Pyspark-Filtering::doc}{{1.4}{8}{Dataframe Filitering and Operations with Pyspark}{section.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Filtering on values in a column}{8}{subsection.1.4.1}}
\newlabel{2020-08-19-Pyspark-Filtering:filtering-on-values-in-a-column}{{1.4.1}{8}{Filtering on values in a column}{subsection.1.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Filtering on values in 2+ columns}{9}{subsection.1.4.2}}
\newlabel{2020-08-19-Pyspark-Filtering:filtering-on-values-in-2-columns}{{1.4.2}{9}{Filtering on values in 2+ columns}{subsection.1.4.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Dataframes, Formatting, Casting Data Type and Correlation with Pyspark}{9}{section.1.5}}
\newlabel{2020-08-20-Pyspark-Dataframes-Data-Types:dataframes-formatting-casting-data-type-and-correlation-with-pyspark}{{1.5}{9}{Dataframes, Formatting, Casting Data Type and Correlation with Pyspark}{section.1.5}{}}
\newlabel{2020-08-20-Pyspark-Dataframes-Data-Types::doc}{{1.5}{9}{Dataframes, Formatting, Casting Data Type and Correlation with Pyspark}{section.1.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.1}Casting Data Types and Formatting Significant Digits}{10}{subsection.1.5.1}}
\newlabel{2020-08-20-Pyspark-Dataframes-Data-Types:casting-data-types-and-formatting-significant-digits}{{1.5.1}{10}{Casting Data Types and Formatting Significant Digits}{subsection.1.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.2}New Columns generated from extant columns using withColumn}{10}{subsection.1.5.2}}
\newlabel{2020-08-20-Pyspark-Dataframes-Data-Types:new-columns-generated-from-extant-columns-using-withcolumn}{{1.5.2}{10}{New Columns generated from extant columns using withColumn}{subsection.1.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.3}Finding the Mean, Max, and Min}{10}{subsection.1.5.3}}
\newlabel{2020-08-20-Pyspark-Dataframes-Data-Types:finding-the-mean-max-and-min}{{1.5.3}{10}{Finding the Mean, Max, and Min}{subsection.1.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.4}Finding the max value by Year}{11}{subsection.1.5.4}}
\newlabel{2020-08-20-Pyspark-Dataframes-Data-Types:finding-the-max-value-by-year}{{1.5.4}{11}{Finding the max value by Year}{subsection.1.5.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}RDDs and Schemas and Data Types with Pyspark}{11}{section.1.6}}
\newlabel{2020-08-21-RDDs and Schemas and Data Types with Pyspark:rdds-and-schemas-and-data-types-with-pyspark}{{1.6}{11}{RDDs and Schemas and Data Types with Pyspark}{section.1.6}{}}
\newlabel{2020-08-21-RDDs and Schemas and Data Types with Pyspark::doc}{{1.6}{11}{RDDs and Schemas and Data Types with Pyspark}{section.1.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.1}Setting Data Schema and Data Types}{12}{subsection.1.6.1}}
\newlabel{2020-08-21-RDDs and Schemas and Data Types with Pyspark:setting-data-schema-and-data-types}{{1.6.1}{12}{Setting Data Schema and Data Types}{subsection.1.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.2}Applying the Data Schema/Data Types while reading in a CSV}{12}{subsection.1.6.2}}
\newlabel{2020-08-21-RDDs and Schemas and Data Types with Pyspark:applying-the-data-schema-data-types-while-reading-in-a-csv}{{1.6.2}{12}{Applying the Data Schema/Data Types while reading in a CSV}{subsection.1.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.3}Using select with RDDs}{13}{subsection.1.6.3}}
\newlabel{2020-08-21-RDDs and Schemas and Data Types with Pyspark:using-select-with-rdds}{{1.6.3}{13}{Using select with RDDs}{subsection.1.6.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.4}Renaming Columns using withColumnRenamed}{13}{subsection.1.6.4}}
\newlabel{2020-08-21-RDDs and Schemas and Data Types with Pyspark:renaming-columns-using-withcolumnrenamed}{{1.6.4}{13}{Renaming Columns using withColumnRenamed}{subsection.1.6.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.5}New Columns by Transforming extant Columns using withColumn}{13}{subsection.1.6.5}}
\newlabel{2020-08-21-RDDs and Schemas and Data Types with Pyspark:new-columns-by-transforming-extant-columns-using-withcolumn}{{1.6.5}{13}{New Columns by Transforming extant Columns using withColumn}{subsection.1.6.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.6}Spark SQL for SQL functionality using createOrReplaceTempView}{13}{subsection.1.6.6}}
\newlabel{2020-08-21-RDDs and Schemas and Data Types with Pyspark:spark-sql-for-sql-functionality-using-createorreplacetempview}{{1.6.6}{13}{Spark SQL for SQL functionality using createOrReplaceTempView}{subsection.1.6.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Window functions and Pivot Tables with Pyspark}{13}{section.1.7}}
\newlabel{2020-08-22-Window functions and Pivot Tables with Pyspark:window-functions-and-pivot-tables-with-pyspark}{{1.7}{13}{Window functions and Pivot Tables with Pyspark}{section.1.7}{}}
\newlabel{2020-08-22-Window functions and Pivot Tables with Pyspark::doc}{{1.7}{13}{Window functions and Pivot Tables with Pyspark}{section.1.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.1}Resilient Distributed Datasets}{13}{subsection.1.7.1}}
\newlabel{2020-08-22-Window functions and Pivot Tables with Pyspark:resilient-distributed-datasets}{{1.7.1}{13}{Resilient Distributed Datasets}{subsection.1.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.2}Using toPandas to look at the data}{14}{subsection.1.7.2}}
\newlabel{2020-08-22-Window functions and Pivot Tables with Pyspark:using-topandas-to-look-at-the-data}{{1.7.2}{14}{Using toPandas to look at the data}{subsection.1.7.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.3}Renaming Columns}{14}{subsection.1.7.3}}
\newlabel{2020-08-22-Window functions and Pivot Tables with Pyspark:renaming-columns}{{1.7.3}{14}{Renaming Columns}{subsection.1.7.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.4}Selecting Columns of Interest}{15}{subsection.1.7.4}}
\newlabel{2020-08-22-Window functions and Pivot Tables with Pyspark:selecting-columns-of-interest}{{1.7.4}{15}{Selecting Columns of Interest}{subsection.1.7.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.5}Sorting RDDs by Columns}{15}{subsection.1.7.5}}
\newlabel{2020-08-22-Window functions and Pivot Tables with Pyspark:sorting-rdds-by-columns}{{1.7.5}{15}{Sorting RDDs by Columns}{subsection.1.7.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.6}Casting Data Types}{15}{subsection.1.7.6}}
\newlabel{2020-08-22-Window functions and Pivot Tables with Pyspark:casting-data-types}{{1.7.6}{15}{Casting Data Types}{subsection.1.7.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.7}Aggregating using groupBy, .agg and sum/max}{15}{subsection.1.7.7}}
\newlabel{2020-08-22-Window functions and Pivot Tables with Pyspark:aggregating-using-groupby-agg-and-sum-max}{{1.7.7}{15}{Aggregating using groupBy, .agg and sum/max}{subsection.1.7.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.8}Exponentials using exp}{15}{subsection.1.7.8}}
\newlabel{2020-08-22-Window functions and Pivot Tables with Pyspark:exponentials-using-exp}{{1.7.8}{15}{Exponentials using exp}{subsection.1.7.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.9}Window functions}{16}{subsection.1.7.9}}
\newlabel{2020-08-22-Window functions and Pivot Tables with Pyspark:window-functions}{{1.7.9}{16}{Window functions}{subsection.1.7.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.10}Lagging Variables}{16}{subsection.1.7.10}}
\newlabel{2020-08-22-Window functions and Pivot Tables with Pyspark:lagging-variables}{{1.7.10}{16}{Lagging Variables}{subsection.1.7.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.11}Looking at windows within the data}{16}{subsection.1.7.11}}
\newlabel{2020-08-22-Window functions and Pivot Tables with Pyspark:looking-at-windows-within-the-data}{{1.7.11}{16}{Looking at windows within the data}{subsection.1.7.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.12}Pivot Dataframes}{16}{subsection.1.7.12}}
\newlabel{2020-08-22-Window functions and Pivot Tables with Pyspark:pivot-dataframes}{{1.7.12}{16}{Pivot Dataframes}{subsection.1.7.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.13}Unpivoting RDDs}{17}{subsection.1.7.13}}
\newlabel{2020-08-22-Window functions and Pivot Tables with Pyspark:unpivoting-rdds}{{1.7.13}{17}{Unpivoting RDDs}{subsection.1.7.13}{}}
\bibcite{intro:kearney2019ties}{Kea19}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{19}{chapter*.5}}
